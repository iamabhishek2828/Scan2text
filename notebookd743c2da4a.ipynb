{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9386853,"sourceType":"datasetVersion","datasetId":5695494},{"sourceId":9389210,"sourceType":"datasetVersion","datasetId":5697247},{"sourceId":9393162,"sourceType":"datasetVersion","datasetId":5700478}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\na=0\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        a+=1\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-14T04:40:56.860264Z","iopub.execute_input":"2024-09-14T04:40:56.860699Z","iopub.status.idle":"2024-09-14T04:41:20.907323Z","shell.execute_reply.started":"2024-09-14T04:40:56.860648Z","shell.execute_reply":"2024-09-14T04:41:20.906147Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"pip install transformers torch torchvision datasets pillow","metadata":{"execution":{"iopub.status.busy":"2024-09-14T09:22:11.249271Z","iopub.execute_input":"2024-09-14T09:22:11.250112Z","iopub.status.idle":"2024-09-14T09:22:24.501453Z","shell.execute_reply.started":"2024-09-14T09:22:11.250061Z","shell.execute_reply":"2024-09-14T09:22:24.500265Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.44.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.19.0)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.21.0)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (9.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.24.6)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.4)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-09-14T10:08:04.364703Z","iopub.execute_input":"2024-09-14T10:08:04.365621Z","iopub.status.idle":"2024-09-14T10:08:04.369893Z","shell.execute_reply.started":"2024-09-14T10:08:04.365559Z","shell.execute_reply":"2024-09-14T10:08:04.368895Z"},"trusted":true},"outputs":[],"execution_count":21},{"cell_type":"code","source":"import gc\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-09-14T10:08:05.119899Z","iopub.execute_input":"2024-09-14T10:08:05.120784Z","iopub.status.idle":"2024-09-14T10:08:05.606838Z","shell.execute_reply.started":"2024-09-14T10:08:05.120741Z","shell.execute_reply":"2024-09-14T10:08:05.605868Z"},"trusted":true},"outputs":[],"execution_count":22},{"cell_type":"code","source":"import os","metadata":{"execution":{"iopub.status.busy":"2024-09-14T10:08:06.652912Z","iopub.execute_input":"2024-09-14T10:08:06.653554Z","iopub.status.idle":"2024-09-14T10:08:06.657528Z","shell.execute_reply.started":"2024-09-14T10:08:06.653516Z","shell.execute_reply":"2024-09-14T10:08:06.656555Z"},"trusted":true},"outputs":[],"execution_count":23},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nfrom torch.utils.data import Dataset, DataLoader\n\n# Load dataset (assumed to be a CSV file)\ndf = pd.read_csv(\"/kaggle/input/dataasa/valid_data.csv\")\n\n# Filter dataset to only include 10 rows per entity_name\nsampled_df = df\n\n# Define a custom dataset for loading images and text\nclass ImageTextDataset(Dataset):\n    def __init__(self, dataframe, processor):\n        self.dataframe = dataframe\n        self.processor = processor\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        # Load the image\n        image_path = self.dataframe.loc[idx, \"image_path\"]\n        image = Image.open(image_path).convert(\"RGB\")\n        \n        # Create the text input by combining group_id and entity_name\n        group_id = self.dataframe.loc[idx, \"group_id\"]\n        entity_name = self.dataframe.loc[idx, \"entity_name\"]\n        text_input = f\"Group ID: {group_id}, Entity Name: {entity_name}, generate just the entity value with the unit and nothing else.\"\n        \n        # The target is the entity_value (the value we want to predict)\n        entity_value = self.dataframe.loc[idx, \"entity_value\"]\n        \n        # Process the image and text input\n        inputs = self.processor(images=image, text=text_input, padding=\"max_length\", return_tensors=\"pt\", max_length=50, truncation=True)\n        \n        # Process the target (entity_value) as input IDs, with attention mask\n        target_inputs = self.processor(text=entity_value, padding=\"max_length\", return_tensors=\"pt\", max_length=50, truncation=True)\n\n        # Return processed inputs and target input IDs with attention mask\n        return inputs, target_inputs['input_ids'], target_inputs['attention_mask']\n\n# Initialize the BLIP processor (for image and text)\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n\n# Create the dataset\ndataset = ImageTextDataset(sampled_df, processor)\n\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader\n\n# Update collate function to handle padding for both image and text inputs, and targets\ndef collate_fn(batch):\n    inputs = {\n        'input_ids': [],\n        'pixel_values': [],\n        'attention_mask': []\n    }\n    targets = {\n        'input_ids': [],\n        'attention_mask': []\n    }\n\n    for item in batch:\n        input_data, target_ids, target_mask = item\n        inputs['input_ids'].append(input_data['input_ids'].squeeze(0))\n        inputs['pixel_values'].append(input_data['pixel_values'].squeeze(0))\n        inputs['attention_mask'].append(input_data['attention_mask'].squeeze(0))\n        targets['input_ids'].append(target_ids.squeeze(0))\n        targets['attention_mask'].append(target_mask.squeeze(0))\n\n    # Pad input_ids and attention_mask to the same length\n    inputs['input_ids'] = pad_sequence(inputs['input_ids'], batch_first=True, padding_value=processor.tokenizer.pad_token_id)\n    inputs['attention_mask'] = pad_sequence(inputs['attention_mask'], batch_first=True, padding_value=0)\n\n    # Pad target input_ids and attention_mask to the same length\n    targets['input_ids'] = pad_sequence(targets['input_ids'], batch_first=True, padding_value=processor.tokenizer.pad_token_id)\n    targets['attention_mask'] = pad_sequence(targets['attention_mask'], batch_first=True, padding_value=0)\n\n    # Stack pixel values\n    inputs['pixel_values'] = torch.stack(inputs['pixel_values'])\n\n    return inputs, targets\n\n# Create DataLoader with the custom collate function\ndataloader = DataLoader(dataset, batch_size=5000, shuffle=True, collate_fn=collate_fn)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-14T10:10:04.734839Z","iopub.execute_input":"2024-09-14T10:10:04.735221Z","iopub.status.idle":"2024-09-14T10:10:05.834555Z","shell.execute_reply.started":"2024-09-14T10:10:04.735186Z","shell.execute_reply":"2024-09-14T10:10:05.833728Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"from transformers import AdamW\nimport torch.nn as nn\ncheckpoint_dir = '/kaggle/working/checkpoints'\nos.makedirs(checkpoint_dir, exist_ok=True)\n\n# Initialize the BLIP model for conditional generation\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nmodel.train()\n\n# Define optimizer\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\n# Move the model to GPU (if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\ncheckpoint_file = os.path.join(checkpoint_dir, 'best_model.pth')\n# Training loop\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        inputs, targets = batch\n        \n        # Move inputs and targets to GPU (if available)\n        input_ids = inputs['input_ids'].to(device)\n        pixel_values = inputs['pixel_values'].to(device)\n        attention_mask = inputs['attention_mask'].to(device)\n        target_ids = targets['input_ids'].to(device)\n        target_attention_mask = targets['attention_mask'].to(device)\n        \n        # Forward pass\n        outputs = model(\n            pixel_values=pixel_values,\n            input_ids=input_ids,\n            labels=target_ids,\n            attention_mask=attention_mask,\n            #decoder_attention_mask=target_attention_mask\n        )\n        \n        # Compute loss\n        loss = outputs.loss\n        \n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    torch.save(model.state_dict(), checkpoint_file)\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")\nmodel.load_state_dict(torch.load(checkpoint_file))","metadata":{"execution":{"iopub.status.busy":"2024-09-14T10:10:15.002359Z","iopub.execute_input":"2024-09-14T10:10:15.003074Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"def predict_entity_value(image_path, group_id, entity_name, model, processor, device):\n    # Load the image\n    image = Image.open(image_path).convert(\"RGB\")\n    \n    # Create the text input by combining group_id and entity_name\n    text_input = f\"Group ID: {group_id}, Entity Name: {entity_name}, generate just the entity value with the unit and nothing else.\"\n    \n    # Process the image and text input\n    inputs = processor(images=image, text=text_input, return_tensors=\"pt\", padding=\"max_length\", max_length=50, truncation=True)\n    \n    # Move inputs to the correct device (GPU or CPU)\n    pixel_values = inputs[\"pixel_values\"].to(device)\n    input_ids = inputs[\"input_ids\"].to(device)\n    attention_mask = inputs[\"attention_mask\"].to(device)\n    \n    # Generate predictions\n    with torch.no_grad():\n        generated_ids = model.generate(\n            pixel_values=pixel_values,\n            max_length=50,  # Maximum length for generated entity_value\n            num_beams=5,    # Number of beams for beam search\n            early_stopping=True\n        )\n    \n    # Decode the generated tokens into text (predicted entity_value)\n    predicted_entity_value = processor.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n    \n    return predicted_entity_value\n\n\nmodel.eval()\n# Example inference\nimage_path = \"/kaggle/input/random-dataset/maximum_weight_recommendation-20240913T170711Z-001/maximum_weight_recommendation/6135h5ZfarL.jpg\"\n\ngroup_id = \"801829\"\nentity_name = \"maximum_weight_recommendation\"\n\npredicted_value = predict_entity_value(image_path, group_id, entity_name, model, processor, device)\nprint(f\"Predicted entity value: {predicted_value}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-14T09:57:18.459614Z","iopub.execute_input":"2024-09-14T09:57:18.460621Z","iopub.status.idle":"2024-09-14T09:57:19.625194Z","shell.execute_reply.started":"2024-09-14T09:57:18.460578Z","shell.execute_reply":"2024-09-14T09:57:19.624269Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Predicted entity value: 100 gram gram\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"model.save_pretrained(\"/path/to/save/fine-tuned-blip\")\nprocessor.save_pretrained(\"/path/to/save/fine-tuned-blip\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set the model to evaluation mode\nmodel.eval()\n\n# Example inference function\ndef generate_entity_value(image_path, prompt_text, max_length=50):\n    # Load and preprocess the image\n    image = Image.open(image_path).convert(\"RGB\")\n    \n    # Process the image and text prompt\n    inputs = processor(images=image, text=prompt_text, return_tensors=\"pt\").to(device)\n\n    # Use the model to generate tokens autoregressively\n    generated_ids = model.generate(\n        pixel_values=inputs['pixel_values'],\n        input_ids=inputs['input_ids'],\n        attention_mask=inputs['attention_mask'],\n        max_length=max_length,\n        num_beams=5,   # You can adjust this for better generation quality\n        early_stopping=True\n    )\n\n    # Decode the generated tokens into text\n    generated_text = processor.decode(generated_ids[0], skip_special_tokens=True)\n    \n    return generated_text\n\n# Example usage\nimage_path = \"/kaggle/input/random-dataset/voltage-20240913T163556Z-001/voltage610u3xH0MfL.jpg\"\nprompt_text = \"Group ID: 271537, Entity Name: voltage\"\n\n# Generate the entity value\ngenerated_value = generate_entity_value(image_path, prompt_text)\nprint(\"Generated Entity Value:\", generated_value)","metadata":{"execution":{"iopub.status.busy":"2024-09-14T08:16:28.262339Z","iopub.execute_input":"2024-09-14T08:16:28.262868Z","iopub.status.idle":"2024-09-14T08:16:58.080345Z","shell.execute_reply.started":"2024-09-14T08:16:28.262823Z","shell.execute_reply":"2024-09-14T08:16:58.079091Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Generated Entity Value: group id : 271537, entity name : voltage\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}